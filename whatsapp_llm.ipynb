{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-edtJume8cIc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install xformers --upgrade\n",
        "\n",
        "import os\n",
        "os.environ['TRITON_DISABLE_LINE_INFO'] = '1' # Optional, tested with and without\n",
        "os.environ['TRITON_JIT_DISABLE_OPT'] = '1' # Likely the most critical change"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up and downloading pre trained model"
      ],
      "metadata": {
        "id": "o8bFuUoiAEbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Constants\n",
        "person_model = \"Alex oh\"\n",
        "pretrained_model = \"unsloth/mistral-7b-bnb-4bit\"\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = pretrained_model,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "NMqayMY6Ca2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Lora Adapter"
      ],
      "metadata": {
        "id": "rDy0OgkyAI2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "qwhqvHPb__HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parses a whatsapp style chat log into documents"
      ],
      "metadata": {
        "id": "3Kx3op2C2rJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data set parse\n",
        "def parseLine(line):\n",
        "    try:\n",
        "        name = line[line.index(']') + 2: line.index(': ')]\n",
        "        msg = line[line.index(': ') + 2:]\n",
        "        return name, msg\n",
        "    except:\n",
        "        return None, line.strip()\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, messages):\n",
        "        self.messages = messages\n",
        "\n",
        "    def to_dict(self):\n",
        "        messages_dict = [{\"from\": name, \"value\": msg} for name, msg in self.messages]\n",
        "        return messages_dict\n",
        "\n",
        "def big_chunk():\n",
        "    MAX = 8\n",
        "    documents = []\n",
        "\n",
        "    with open('_chat.txt', 'r') as file:\n",
        "        prev_name = None\n",
        "        prev_msg = ''\n",
        "\n",
        "        curr_messages = []\n",
        "\n",
        "        for line in file:\n",
        "            name, msg = parseLine(line)\n",
        "            msg = msg.replace('\\n', ' ').strip()\n",
        "\n",
        "            # Skip system messages\n",
        "            if '\\u200e' in msg or msg.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            # first messgage\n",
        "            if prev_name is None:\n",
        "                prev_name = name\n",
        "\n",
        "            # continuation\n",
        "            if name is None or name == prev_name:\n",
        "                prev_msg += \" \" + msg\n",
        "\n",
        "            # new name\n",
        "            if name != prev_name:\n",
        "                if prev_name == person_model:\n",
        "                    curr_messages.append([prev_name, prev_msg])\n",
        "                else:\n",
        "                    curr_messages.append(['user', f'{prev_name}: {prev_msg}'])\n",
        "                if len(curr_messages) >= MAX and prev_name == person_model:\n",
        "                    document = Document(curr_messages)\n",
        "                    curr_messages = []\n",
        "                    documents.append(document)\n",
        "\n",
        "                prev_name = name\n",
        "                prev_msg = msg\n",
        "\n",
        "        if len(curr_messages) > 0:\n",
        "            documents.append(Document(curr_messages))\n",
        "\n",
        "    return documents\n",
        "\n",
        "documents = big_chunk()\n",
        "\n",
        "# convert to json\n",
        "documents = [document.to_dict() for document in documents]"
      ],
      "metadata": {
        "id": "Q7zz2o5eCsHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "unsloth_template = (\n",
        "  \"{{ bos_token }}\"\\\n",
        "  \"{{ 'You are Alex oh the assistant, maintain conversation as they would\\\\n' }}\"\\\n",
        "  \"{% for message in messages %}\"\\\n",
        "      \"{% if message['from'] == 'user' %}\"\\\n",
        "          \"{{ '>>> User: ' + message['value'] + '\\\\n' }}\"\\\n",
        "      \"{% elif message['from'] == 'Alex oh' %}\"\\ # Change this name to person_model\n",
        "          \"{{ '>>> Assistant: ' + message['value'] + '\\\\n' }}\"\\\n",
        "      \"{% endif %}\"\\\n",
        "  \"{% endfor %}\"\\\n",
        "  \"{% if add_generation_prompt %}\"\\\n",
        "      \"{{ '>>> Assistant: ' }}\"\\\n",
        "  \"{% endif %}\"\\\n",
        "  \"{{ eos_token }}\"\\\n",
        ")\n",
        "\n",
        "unsloth_eos_token = \"eos_token\"\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = (unsloth_template, \"eos_token\"),\n",
        "    mapping = {\n",
        "        \"role\": \"from\",\n",
        "        \"content\": \"value\",\n",
        "        \"user\": \"user\",    # Who provides the input\n",
        "        \"assistant\": person_model     # Who gives the output to learn\n",
        "    },\n",
        "    map_eos_token = True\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]  # <- use batch of convos\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    print(\"Texts generated:\", len(texts))\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "dataset = Dataset.from_list([{\"conversations\": doc} for doc in documents])\n",
        "\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# For testing of dataset\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "q6gdY6Fw7D7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = False,\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to='none',\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "T73bxL5hNG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download for local use"
      ],
      "metadata": {
        "id": "D8JxMT_uqGtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "  from peft import PeftModel\n",
        "  from transformers import AutoModelForCausalLM\n",
        "\n",
        "  trainer.model.save_pretrained(\"outputs\")\n",
        "  tokenizer.save_pretrained(\"outputs\")\n",
        "\n",
        "  base_model = AutoModelForCausalLM.from_pretrained(pretrained_model)  # or your base model\n",
        "  model = PeftModel.from_pretrained(base_model, \"outputs\")\n",
        "  model = model.merge_and_unload()  # Merge LoRA into base model\n",
        "  model.save_pretrained(\"merged_model\")\n",
        "\n",
        "  !zip -r outputs.zip merged_model/\n",
        "  from google.colab import files\n",
        "  files.download(\"outputs.zip\")\n"
      ],
      "metadata": {
        "id": "216J8Vc9RQAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "8t0EZO_vIbJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves the model to be called for testing\n",
        "if False:\n",
        "  trainer.model.save_pretrained(\"outputs\")\n",
        "  tokenizer.save_pretrained(\"outputs\")\n",
        "\n",
        "  !zip -r outputs.zip outputs/\n",
        "  from google.colab import files\n",
        "  files.download(\"outputs.zip\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dhHo30O6HS7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Load the model using Unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"outputs\",  # Your saved model folder\n",
        "    max_seq_length = 1024,   # Or whatever you used\n",
        "    dtype = None,            # Or torch.float16 / bfloat16 if needed\n",
        "    load_in_4bit = True,     # Or False depending on your setup\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "282b2lVtHb0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, max_new_tokens=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "cVm8tuzIImkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it\n",
        "response = generate_response('''<s>You are Alex oh the assistant, maintain conversation as they would, reply as the assitant only\n",
        ">>> User: shall we go to the beach tmr\n",
        ">>> Assistant:''')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "3x9uyowxrAVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}